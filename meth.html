<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Methodology - Data Showdown</title>
    <style>
        /* Reset default margin and padding */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        /* Body styles */
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            color: #333;
            line-height: 1.6;
            margin: 0;
            padding: 0;
        }

        /* Container styles */
        .container {
            display: flex;
        }

        /* Navigation styles */
        nav {
            background-color: #333;
            color: #fff;
            width: 200px; /* Width of the navigation */
            padding: 20px;
        }

        nav ul {
            list-style-type: none;
            padding: 0;
            margin: 0;
        }

        nav ul li {
            margin-bottom: 10px;
        }

        nav ul li a {
            color: #fff;
            text-decoration: none;
            display: block;
            padding: 8px;
            border-radius: 4px;
        }

        nav ul li a:hover {
            background-color: #555;
        }

        /* Main content styles */
        .main-content {
            padding: 20px;
        }

        header {
            background-color: red; 
            color: #fff;
            text-align: center;
            padding: 20px 0;
            margin-bottom: 20px;
        }
    </style>
</head>

<body>
    <header>
        <h1>Comparing machine learning performance on Big Query, Azure and Python</h1>
    </header>
    <div class="container">
        <nav>
            <ul>
                <li><a href="index.html">Introduction</a></li>
                <li><a href="des.html">Description</a></li>
                <li><a href="meth.html">Methodology</a></li>
                <li><a href="res.html">Results</a></li>
                <li><a href="con.html">Conclusion</a></li>
                <li><a href="about.html">About Us</a></li>
            </ul>
        </nav>
        <div class="main-content">
            <h2>Methodology</h2>
            <!-- Content for the Methodology page goes here -->
            <p>For this study we choose our dataset from Kaggle (Housing Prices Dataset, n.d.) which is based on Real State data. The dataset has 546 rows and 13 columns. The features in this dataset are the following:
                •	price 
                •	Price of houses(numeric)
                •	area
                •	The size of the area(numeric)
                •	bedrooms
                •	Number of bedrooms(numeric)
                •	bathrooms
                •	Number of bathrooms(numeric)
                •	stories
                •	Number of stories(numeric)
                •	mainroad
                •	Binary answer, Yes or No, to indicate if the house has main road.
                •	guestroom
                •	Binay answer, Yes or No, to indicate if the house has guestroom.
                •	basement
                •	Binary answer, Yes or No to indicate if the house has basement.
                •	hotwaterheating
                •	Binary answer, Yes or No to indicate if the house has hotwater heating.
                •	airconditioning
                •	Binary answer, Yes or No to indicate if the house has air conditioning.
                •	parking
                •	Number of parking spaces(numeric).
                •	prefarea
                •	Binary answer, Yes or No to indicate if the house has prefarea.
                •	furnishingstatus
                •	3 main categorical answers, furnished,semi-furnished and unfurnished to indicate the status of house furnishment.
                We used this dataset on Google Big Query, Azure Machine learning Studio and Jupyter NoteBook. On all of these platforms we applied Linear Regression, XGBoost and LightGBM and tried to compare the performance of the models.
                Google Big Query:
                1.	Uploading Dataset
                First, we will go to console.cloud.google.com and pick Big Query from our left tab and choose Big Query. Form here we go Add>Local File>Uplaod>Create Table.
                2.	Linear Regression
First we need to know what our project id, dataset id and table id is. In our case it is “realstate-data.Data_Set.Housing”. this is the format we need to remember when we use big query. 
We referenced the above format to create a model. We used linea_reg as the model type and used price as the target variable while selecting all the columns in the dataset. We also evaluated the model using “ML.Evaluate”
3.	XGBoost
With the same reference, we will create a model using model type as BOOSTED_TREE_REGRESSOR with bosster type GBTREE. With tree iterations as 100 and input label as price, we make the model by selecting all the columns in the dataset. Later we evaluate the model using ML.Evaluate.

4.	lightGBM
Unfortunately unlike XGBoost, Big query does not have any option to use lightGBM. So we did not use Big Query for it.
Azure Machine Learning Studio:
1.	Upload dataset
First we need to go to Azure AI Machine Learning Studio and select designer tab on the left. We will have to create a new pipeline in order to design a model. Then just go to data section and upload the data from the local disk. In our case the dataset is saved as real_state_data.

2.	Linear Regression
First we drag and dropped the dataset we uploaded, which was “real_state_data”. Now we went to component section and used “Select columns in datset” where we choose all the columns from the datset. Then we will select “clean missing data” and include all the columns other than price. “split data” where we use 0.7 as training set.  This will connect to “train data” where we use the label column as price. Next we add “linear regression” model and connect it with “train model”. We will add “score model” which will have connection to “train model” and “split data”. finally we add “evaluate model” to finish the design.

Then we press configure and submit to run the model.
3.	XGBoost
Unfortunately XGBoost is not directly available in Azure Machine Learning Studio.
4.	LighGBM
The process is almost the same. Few things that are different are, we added “apply math operation” before “split data” so that we can use log on price column. Instead of linear regression we used “Boosted Decision Tree Regression” where we configured the following; Maximum number of leaves per tree = 4, Minimum number of samples per leaf node = 10, Learning rate = 0.1, Total number of trees constructed = 100 and Random number seed = 42.
Importing the Kaggle dataset into Big Query
1.	Install the Cloud SDK (https://cloud.google.com/sdk/docs/install-sdk)
2.	Create a Google Cloud Project (https://console.cloud.google.com/projectcreate?referrer=search)
3.	Create a Storage bucket (https://console.cloud.google.com/storage/create-bucket)
4.	Select created bucket and upload the dataset.
5.	Go to Big Query (https://console.cloud.google.com/bigquery?hl=en&pro)
•	Create dataset.
•	Select the created dataset and create table.
•	Select the created table and choose goggle cloud storage as data source
•	Lastly select the dataset from the created storage bucket.

Training the regression model with XGBoost

We started by creating a python notebook on Big Query, then we loaded the dataset we imported from the GCS bucket. We analysed the data to check for missing values and outliers. We preprocessed the data by using “OneHotEncoder” to convert the categorical features into binary.
We also log-transformed the target variable due to the presence of outliers. We used the grid-search approach for hyperparameter tuning to determine the optimal values of the parameters in our xgboost algorithm. Finally, we used the r-squared metric for the model evaluation. 

For the model deployment, we first looked at AWS Sagemaker and we did the following: 
1.	Preparing the model files
 We started by saving the xgboost regression model as a .joblib file. Then we created a custom inference script named predictor.py. We then packaged the model.joblib file and the predictor.py file into a .tar.gz file using the command “tar -czvf model.tar.gz model.joblib”

2.	Uploading the .tar.gz file to Amazon S3
We used the following command to copy the  .tar.gz file to the s3 bucket we created. 
“aws s3 cp model.tar.gz s3://bigdataprojectt/”

3.	Building a docker image
We built a docker image locally with a Dockerfile using the following command “docker build -t my-model-predictor .” in the directory of the dockerfile. We then used the “docker push gcr.io/house-pred-420306/my-model-predictor:latest” command to push the image to an Amazon Elastic Container Registry (ECR). AWS ECR allows us to store and manage our docker images for use in SageMaker.

4.	Creating a SageMaker Model
•	Go to the SageMaker Console.
•	Click on “Model”, then create “create model”
•	Give a model name
•	Select the created IAM role
•	Enter the custom docker container URL, and the s3 path to the model
•	Click “create model”.
5.	Creating an endpoint configuration
•	Go to the SageMaker console.
•	Click on “Endpoint configurations”, then “Create endpoint configuration”.
•	Give it a name.
•	Add the created model to the configuration.
•	Create endpoint configuration.

6.	Creating and deploying the endpoint
•	Go to the SageMaker console.
•	Click on “Endpoints” then “Create endpoint”.
•	Enter an endpoint name.
•	Select the created endpoint configuration.
•	Click “Create endpoint”
This ran for hours, but unfortunately later failed. We ran a lot of stuff but couldn’t make it work.  
After AWS SageMaker failed, we then tried to use GCP Vertex AI for the deployment. Using Vertex AI, we were also able to get everything we did right in AWS done. From preparing the model file to creating the docker image and running the flask app. Deploying the model to the endpoint is also where we failed here.
Importing the Kaggle dataset into Big Query
6.	Install the Cloud SDK (https://cloud.google.com/sdk/docs/install-sdk)
7.	Create a Google Cloud Project (https://console.cloud.google.com/projectcreate?referrer=search)
8.	Create a Storage bucket (https://console.cloud.google.com/storage/create-bucket)
9.	Select created bucket and upload the dataset.
10.	Go to Big Query (https://console.cloud.google.com/bigquery?hl=en&pro)
•	Create dataset.
•	Select the created dataset and create table.
•	Select the created table and choose goggle cloud storage as data source
•	Lastly select the dataset from the created storage bucket.

Training the regression model with XGBoost
We started by creating a python notebook on Big Query, then we loaded the dataset we imported from the GCS bucket. We analysed the data to check for missing values and outliers. We preprocessed the data by using “OneHotEncoder” to convert the categorical features into binary.
We also log-transformed the target variable due to the presence of outliers. We used the grid-search approach for hyperparameter tuning to determine the optimal values of the parameters in our xgboost algorithm. Finally, we used the r-squared metric for the model evaluation. 

For the model deployment, we first looked at AWS Sagemaker and we did the following: 

7.	Preparing the model files
 We started by saving the xgboost regression model as a .joblib file. Then we created a custom inference script named predictor.py. We then packaged the model.joblib file and the predictor.py file into a .tar.gz file using the command “tar -czvf model.tar.gz model.joblib”

8.	Uploading the .tar.gz file to Amazon S3
We used the following command to copy the  .tar.gz file to the s3 bucket we created. 
“aws s3 cp model.tar.gz s3://bigdataprojectt/”

9.	Building a docker image
We built a docker image locally with a Dockerfile using the following command “docker build -t my-model-predictor .” in the directory of the dockerfile. We then used the “docker push gcr.io/house-pred-420306/my-model-predictor:latest” command to push the image to an Amazon Elastic Container Registry (ECR). AWS ECR allows us to store and manage our docker images for use in SageMaker.
10.	Creating a SageMaker Model
•	Go to the SageMaker Console.
•	Click on “Model”, then create “create model”
•	Give a model name
•	Select the created IAM role
•	Enter the custom docker container URL, and the s3 path to the model
•	Click “create model”.
11.	Creating an endpoint configuration
•	Go to the SageMaker console.
•	Click on “Endpoint configurations”, then “Create endpoint configuration”.
•	Give it a name.
•	Add the created model to the configuration.
•	Create endpoint configuration.
12.	Creating and deploying the endpoint
•	Go to the SageMaker console.
•	Click on “Endpoints” then “Create endpoint”.
•	Enter an endpoint name.
•	Select the created endpoint configuration.
•	Click “Create endpoint”
This ran for hours, but unfortunately later failed. We ran a lot of stuff but couldn’t make it work.  

After AWS SageMaker failed, we then tried to use GCP Vertex AI for the deployment. Using Vertex AI, we were also able to get everything we did right in AWS done. From preparing the model file to creating the docker image and running the flask app. Deploying the model to the endpoint is also where we failed here.
</p>
        </div>
    </div>
</body>

</html>
